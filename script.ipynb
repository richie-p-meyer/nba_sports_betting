{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0336ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2c6760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEASONS = list(range(2016,2024))\n",
    "\n",
    "if os.path.exists('data'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('data')\n",
    "    os.mkdir(os.path.join('data', 'standings'))\n",
    "    os.mkdir(os.path.join('data', 'scores'))\n",
    "    os.mkdir(os.path.join('data', 'temp'))\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "STANDINGS_DIR = os.path.join(DATA_DIR, \"standings\")\n",
    "SCORES_DIR = os.path.join(DATA_DIR, \"scores\")\n",
    "TEMP_DIR = os.path.join(DATA_DIR, \"temp\")\n",
    "standings_files = os.listdir(STANDINGS_DIR)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout\n",
    "import time\n",
    "# Make sure to install playwright browsers by running playwright install on the command line or !playwright install from Jupyter\n",
    "\n",
    "async def get_html(url, selector, sleep=5, retries=3):\n",
    "    html = None\n",
    "    for i in range(1, retries+1):\n",
    "        time.sleep(sleep * i)\n",
    "        try:\n",
    "            async with async_playwright() as p:\n",
    "                browser = await p.firefox.launch()\n",
    "                page = await browser.new_page()\n",
    "                await page.goto(url)\n",
    "                print(await page.title())\n",
    "                html = await page.inner_html(selector)\n",
    "        except PlaywrightTimeout:\n",
    "            print(f\"Timeout error on {url}\")\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    return html\n",
    "\n",
    "async def scrape_season(season):\n",
    "    url = f\"https://www.basketball-reference.com/leagues/NBA_{season}_games.html\"\n",
    "    html = await get_html(url, \"#content .filter\")\n",
    "    \n",
    "    soup = BeautifulSoup(html)\n",
    "    links = soup.find_all(\"a\")\n",
    "    standings_pages = [f\"https://www.basketball-reference.com{l['href']}\" for l in links]\n",
    "    \n",
    "    for url in standings_pages:\n",
    "        save_path = os.path.join(STANDINGS_DIR, url.split(\"/\")[-1])\n",
    "        if os.path.exists(save_path):\n",
    "            continue\n",
    "        \n",
    "        html = await get_html(url, \"#all_schedule\")\n",
    "        with open(save_path, \"w+\") as f:\n",
    "            f.write(html)\n",
    "            \n",
    "async def scrape_game(standings_file):\n",
    "    with open(standings_file, 'r') as f:\n",
    "        html = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(html)\n",
    "    links = soup.find_all(\"a\")\n",
    "    hrefs = [l.get('href') for l in links]\n",
    "    box_scores = [f\"https://www.basketball-reference.com{l}\" for l in hrefs if l and \"boxscore\" in l and '.html' in l]\n",
    "\n",
    "    for url in box_scores:\n",
    "        save_path = os.path.join(TEMP_DIR, url.split(\"/\")[-1])\n",
    "        check_path_1 = os.path.join(SCORES_DIR, url.split(\"/\")[-1])\n",
    "        check_path_2 = os.path.join(TEMP_DIR, url.split(\"/\")[-1])\n",
    "        if os.path.exists(check_path_1) or os.path.exists(check_path_2):\n",
    "            continue\n",
    "\n",
    "        html = await get_html(url, \"#content\")\n",
    "        if not html:\n",
    "            continue\n",
    "        with open(save_path, \"w+\") as f:\n",
    "            f.write(html)\n",
    "            \n",
    "def parse_html(box_score):\n",
    "    with open(box_score) as f:\n",
    "        html = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(html)\n",
    "    [s.decompose() for s in soup.select(\"tr.over_header\")]\n",
    "    [s.decompose() for s in soup.select(\"tr.thead\")]\n",
    "    return soup\n",
    "\n",
    "def read_season_info(soup):\n",
    "    nav = soup.select(\"#bottom_nav_container\")[0]\n",
    "    hrefs = [a[\"href\"] for a in nav.find_all('a')]\n",
    "    season = os.path.basename(hrefs[1]).split(\"_\")[0]\n",
    "    return season\n",
    "\n",
    "def read_line_score(soup):\n",
    "    line_score = pd.read_html(str(soup), attrs = {'id': 'line_score'})[0]\n",
    "    cols = list(line_score.columns)\n",
    "    cols[0] = \"team\"\n",
    "    cols[-1] = \"total\"\n",
    "    line_score.columns = cols\n",
    "    \n",
    "    line_score = line_score[[\"team\", \"total\"]]\n",
    "    \n",
    "    return line_score\n",
    "\n",
    "def read_stats(soup, team, stat):\n",
    "    df = pd.read_html(str(soup), attrs = {'id': f'box-{team}-game-{stat}'}, index_col=0)[0]\n",
    "    df = df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def read_line_score_test(soup):\n",
    "    teams = soup.select('.scorebox')[0]\n",
    "    hrefs = [t[\"href\"] for t in teams.find_all('a')]\n",
    "    team = [t for t in hrefs if '/teams' in t]\n",
    "    first_team = team[0].split('/')[2]\n",
    "    second_team = team[1].split('/')[2]\n",
    "    scores = soup.find_all('div',{'class':'scores'})\n",
    "    first_score = int(scores[0].text)\n",
    "    second_score =int(scores[1].text)\n",
    "    return pd.DataFrame({'team':[first_team,second_team],'total':[first_score,second_score]})\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5898225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-23 NBA Schedule | Basketball-Reference.com\n",
      "2022-23 NBA Schedule | Basketball-Reference.com\n"
     ]
    }
   ],
   "source": [
    "## Delete old standings file and scrape new with most recent scores\n",
    "month = datetime.now().strftime(\"%B\").lower()\n",
    "if os.path.exists(f'data/standings/NBA_2023_games-{month}.html'):\n",
    "    os.remove(f'data/standings/NBA_2023_games-{month}.html')  ##Update month as needed\n",
    "    await scrape_season(2023)\n",
    "else:\n",
    "    await scrape_season(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adef49c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trail Blazers vs Grizzlies, February 1, 2023 | Basketball-Reference.com\n",
      "Magic vs 76ers, February 1, 2023 | Basketball-Reference.com\n",
      "Nets vs Celtics, February 1, 2023 | Basketball-Reference.com\n",
      "Timeout error on https://www.basketball-reference.com/boxscores/202302010HOU.html\n",
      "Thunder vs Rockets, February 1, 2023 | Basketball-Reference.com\n",
      "Warriors vs Timberwolves, February 1, 2023 | Basketball-Reference.com\n",
      "Kings vs Spurs, February 1, 2023 | Basketball-Reference.com\n",
      "Raptors vs Jazz, February 1, 2023 | Basketball-Reference.com\n",
      "Hawks vs Suns, February 1, 2023 | Basketball-Reference.com\n",
      "Lakers vs Pacers, February 2, 2023 | Basketball-Reference.com\n",
      "Grizzlies vs Cavaliers, February 2, 2023 | Basketball-Reference.com\n",
      "Heat vs Knicks, February 2, 2023 | Basketball-Reference.com\n",
      "Timeout error on https://www.basketball-reference.com/boxscores/202302020CHI.html\n",
      "Hornets vs Bulls, February 2, 2023 | Basketball-Reference.com\n",
      "Pelicans vs Mavericks, February 2, 2023 | Basketball-Reference.com\n",
      "Timeout error on https://www.basketball-reference.com/boxscores/202302020DEN.html\n",
      "Warriors vs Nuggets, February 2, 2023 | Basketball-Reference.com\n",
      "Timeout error on https://www.basketball-reference.com/boxscores/202302020MIL.html\n",
      "Clippers vs Bucks, February 2, 2023 | Basketball-Reference.com\n"
     ]
    }
   ],
   "source": [
    "## run scrape_game function which opens each box score and saves it as an html file - skips if file already exists\n",
    "for season in SEASONS:\n",
    "    files = [s for s in standings_files if str(season) in s]\n",
    "    \n",
    "    for f in files:\n",
    "        filepath = os.path.join(STANDINGS_DIR, f)\n",
    "        \n",
    "        await scrape_game(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adce87de",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_scores = os.listdir(TEMP_DIR)\n",
    "box_scores = [os.path.join(TEMP_DIR, f) for f in box_scores if f.endswith(\".html\")]\n",
    "games = []\n",
    "base_cols = None\n",
    "for box_score in box_scores:\n",
    "    soup = parse_html(box_score)\n",
    "\n",
    "    line_score = read_line_score_test(soup)\n",
    "    teams = list(line_score[\"team\"])\n",
    "\n",
    "    summaries = []\n",
    "    for team in teams:\n",
    "        basic = read_stats(soup, team, \"basic\")\n",
    "        advanced = read_stats(soup, team, \"advanced\")\n",
    "\n",
    "        totals = pd.concat([basic.iloc[-1,:], advanced.iloc[-1,:]])\n",
    "        totals.index = totals.index.str.lower()\n",
    "\n",
    "        maxes = pd.concat([basic.iloc[:-1].max(), advanced.iloc[:-1].max()])\n",
    "        maxes.index = maxes.index.str.lower() + \"_max\"\n",
    "\n",
    "        summary = pd.concat([totals, maxes])\n",
    "        \n",
    "        if base_cols is None:\n",
    "            base_cols = list(summary.index.drop_duplicates(keep=\"first\"))\n",
    "            base_cols = [b for b in base_cols if \"bpm\" not in b]\n",
    "        \n",
    "        summary = summary[base_cols]\n",
    "        \n",
    "        summaries.append(summary)\n",
    "    summary = pd.concat(summaries, axis=1).T\n",
    "\n",
    "    game = pd.concat([summary, line_score], axis=1)\n",
    "\n",
    "    game[\"home\"] = [0,1]\n",
    "\n",
    "    game_opp = game.iloc[::-1].reset_index()\n",
    "    game_opp.columns += \"_opp\"\n",
    "\n",
    "    full_game = pd.concat([game, game_opp], axis=1)\n",
    "    full_game[\"season\"] = read_season_info(soup)\n",
    "    \n",
    "    full_game[\"date\"] = os.path.basename(box_score)[:8]\n",
    "    full_game[\"date\"] = pd.to_datetime(full_game[\"date\"], format=\"%Y%m%d\")\n",
    "    \n",
    "    full_game[\"won\"] = full_game[\"total\"] > full_game[\"total_opp\"]\n",
    "    games.append(full_game)\n",
    "    \n",
    "    if len(games) % 100 == 0:\n",
    "        print(f\"{len(games)} / {len(box_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7d333c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    temp = pd.concat(games, ignore_index=True)\n",
    "    temp.to_csv(\"temp.csv\")\n",
    "    temp = pd.read_csv('temp.csv',index_col=0)\n",
    "except:\n",
    "    temp = pd.read_csv('temp.csv',index_col=0)\n",
    "old = pd.read_csv(\"nba_games_updated.csv\",index_col=0)\n",
    "save = pd.concat([old,temp])\n",
    "save.team = save.team.str.replace('CHO','CHA').str.replace('PHO','PHX').str.replace('BRK','BKN').str.replace('NJN','BKN').str.replace('NOH','NOP')\n",
    "save.to_csv(\"nba_games_updated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0403647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather all files\n",
    "allfiles = os.listdir(TEMP_DIR)\n",
    " \n",
    "# iterate on all files to move them to destination folder\n",
    "for f in allfiles:\n",
    "    src_path = os.path.join(TEMP_DIR, f)\n",
    "    dst_path = os.path.join(SCORES_DIR, f)\n",
    "    os.rename(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cef4a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('next_game.csv',index_col=0)\n",
    "df.DATE = pd.to_datetime(df.DATE)\n",
    "df = df[df.DATE>=datetime.today() - timedelta(days = 1)]\n",
    "df = df.drop_duplicates(subset = 'team', keep='first')\n",
    "df.to_csv('next_game_30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90a12a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('nba_games_updated.csv', index_col=0)\n",
    "df = df.sort_values(\"date\")\n",
    "df = df.reset_index(drop=True)\n",
    "del df[\"mp.1\"]\n",
    "del df[\"mp_opp.1\"]\n",
    "del df[\"index_opp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd37a770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fz/0282wvb93rn0lm_p0nscw1sm0000gn/T/ipykernel_41806/2923568783.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"target\"][pd.isnull(df[\"target\"])] = 2\n"
     ]
    }
   ],
   "source": [
    "def add_target(group):\n",
    "    group[\"target\"] = group[\"won\"].shift(-1)\n",
    "    return group\n",
    "\n",
    "df = df.groupby(\"team\", group_keys=False).apply(add_target)\n",
    "\n",
    "df[\"target\"][pd.isnull(df[\"target\"])] = 2\n",
    "df[\"target\"] = df[\"target\"].astype(int, errors=\"ignore\")\n",
    "\n",
    "nulls = pd.isnull(df).sum()\n",
    "nulls = nulls[nulls > 0]\n",
    "\n",
    "valid_columns = df.columns[~df.columns.isin(nulls.index)]\n",
    "df = df[valid_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd9283ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rr = RidgeClassifier(alpha=1)\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=3, max_samples = .4, n_estimators = 100)\n",
    "\n",
    "split = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "sfs = SequentialFeatureSelector(rf, \n",
    "                                n_features_to_select=30, \n",
    "                                direction=\"backward\",\n",
    "                                cv=split,\n",
    "                                n_jobs=1\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17d7cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_columns = [\"season\", \"date\", \"won\", \"target\", \"team\", \"team_opp\"]\n",
    "selected_columns = df.columns[~df.columns.isin(removed_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74184839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[selected_columns] = scaler.fit_transform(df[selected_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1252e037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(data, model, predictors, target, start=2, step=1):\n",
    "    all_predictions = []\n",
    "    \n",
    "    seasons = sorted(data[\"season\"].unique())\n",
    "    \n",
    "    for i in range(start, len(seasons), step):\n",
    "        season = seasons[i]\n",
    "        train = data[data[\"season\"] < season]\n",
    "        test = data[data[\"season\"] == season]\n",
    "        \n",
    "        model.fit(train[predictors], train[target])\n",
    "        \n",
    "        preds = model.predict(test[predictors])\n",
    "        preds = pd.Series(preds, index=test.index)\n",
    "        combined = pd.concat([test[target], preds], axis=1)\n",
    "        combined.columns = [\"actual\", \"prediction\"]\n",
    "        \n",
    "        all_predictions.append(combined)\n",
    "    return pd.concat(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b769417",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rolling = df[list(selected_columns) + [\"won\", \"team\", \"season\"]]\n",
    "\n",
    "def find_team_averages(team):\n",
    "    rolling = team.rolling(10).mean()\n",
    "    return rolling\n",
    "\n",
    "df_rolling = df_rolling.groupby([\"team\", \"season\"], group_keys=False).apply(find_team_averages)\n",
    "\n",
    "rolling_cols = [f\"{col}_10\" for col in df_rolling.columns]\n",
    "df_rolling.columns = rolling_cols\n",
    "df = pd.concat([df, df_rolling], axis=1)\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2faebe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_col(team, col_name):\n",
    "    next_col = team[col_name].shift(-1)\n",
    "    return next_col\n",
    "\n",
    "def add_col(df, col_name):\n",
    "    return df.groupby(\"team\", group_keys=False).apply(lambda x: shift_col(x, col_name))\n",
    "\n",
    "df[\"home_next\"] = add_col(df, \"home\")\n",
    "df[\"team_opp_next\"] = add_col(df, \"team_opp\")\n",
    "df[\"date_next\"] = add_col(df, \"date\")\n",
    "\n",
    "full = df.merge(df[rolling_cols + [\"team_opp_next\", \"date_next\", \"team\"]], left_on=[\"team\", \"date_next\"], right_on=[\"team_opp_next\", \"date_next\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b7b4dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_columns = list(full.columns[full.dtypes == \"object\"]) + removed_columns\n",
    "selected_columns = full.columns[~full.columns.isin(removed_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45cff4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'rr_model.sav'\n",
    "if os.path.exists(filename):\n",
    "    sfs = pickle.load(open(filename, 'rb'))\n",
    "else:\n",
    "    sfs.fit(full[selected_columns], full[\"target\"])\n",
    "    pickle.dump(sfs, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9274a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = list(selected_columns[sfs.get_support()])\n",
    "predictions = backtest(full, rr, predictors, 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a449c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_game = pd.read_csv('next_game_30.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73915761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to do this before merging into 'Full'\n",
    "df.team = df.team.str.replace('CHO','CHA').str.replace('PHO','PHX').str.replace('BRK','BKN')\n",
    "df.team_opp = df.team_opp.str.replace('CHO','CHA').str.replace('PHO','PHX').str.replace('BRK','BKN')\n",
    "df.team_opp_next = df.team_opp_next.str.replace('CHO','CHA').str.replace('PHO','PHX').str.replace('BRK','BKN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a07ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_30 = df[df.target==2]\n",
    "make_pred = next_30.merge(next_game,how='left',on=['team'])\n",
    "make_pred.home_next = make_pred.HOME\n",
    "make_pred.team_opp_next = make_pred.OPPONENT\n",
    "make_pred.date_next = make_pred.DATE\n",
    "\n",
    "del make_pred['DATE']\n",
    "del make_pred['OPPONENT']\n",
    "del make_pred['HOME']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43180940",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = df[df['target']==2].index\n",
    "df.loc[rows,['target','team','team_opp_next','date_next','home_next']] = np.array(make_pred[['target','team','team_opp_next','date_next','home_next']])\n",
    "full = df.merge(df[rolling_cols + [\"team_opp_next\", \"date_next\", \"team\"]],  left_on=[\"team\", \"date_next\"], right_on=[\"team_opp_next\", \"date_next\"])\n",
    "predictions = backtest(full, rr, predictors,'target')\n",
    "ml = pd.concat([full,predictions],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6085f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = str(datetime.today())\n",
    "today = today[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad33abaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_next</th>\n",
       "      <th>team_x</th>\n",
       "      <th>team_y</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17624</th>\n",
       "      <td>2023-02-03</td>\n",
       "      <td>BOS</td>\n",
       "      <td>PHX</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17639</th>\n",
       "      <td>2023-02-03</td>\n",
       "      <td>CHA</td>\n",
       "      <td>DET</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17618</th>\n",
       "      <td>2023-02-03</td>\n",
       "      <td>DET</td>\n",
       "      <td>CHA</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17631</th>\n",
       "      <td>2023-02-03</td>\n",
       "      <td>MIN</td>\n",
       "      <td>ORL</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17635</th>\n",
       "      <td>2023-02-03</td>\n",
       "      <td>PHI</td>\n",
       "      <td>SAS</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17628</th>\n",
       "      <td>2023-02-03</td>\n",
       "      <td>POR</td>\n",
       "      <td>WAS</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17622</th>\n",
       "      <td>2023-02-03</td>\n",
       "      <td>SAC</td>\n",
       "      <td>IND</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17633</th>\n",
       "      <td>2023-02-03</td>\n",
       "      <td>TOR</td>\n",
       "      <td>HOU</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17630</th>\n",
       "      <td>2023-02-03</td>\n",
       "      <td>UTA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17629</th>\n",
       "      <td>2023-02-03</td>\n",
       "      <td>ATL</td>\n",
       "      <td>UTA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17625</th>\n",
       "      <td>2023-02-03</td>\n",
       "      <td>HOU</td>\n",
       "      <td>TOR</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17637</th>\n",
       "      <td>2023-02-03</td>\n",
       "      <td>IND</td>\n",
       "      <td>SAC</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17634</th>\n",
       "      <td>2023-02-03</td>\n",
       "      <td>ORL</td>\n",
       "      <td>MIN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17627</th>\n",
       "      <td>2023-02-03</td>\n",
       "      <td>PHX</td>\n",
       "      <td>BOS</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17623</th>\n",
       "      <td>2023-02-03</td>\n",
       "      <td>SAS</td>\n",
       "      <td>PHI</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17609</th>\n",
       "      <td>2023-02-03</td>\n",
       "      <td>WAS</td>\n",
       "      <td>POR</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date_next team_x team_y  prediction\n",
       "17624  2023-02-03    BOS    PHX         1.0\n",
       "17639  2023-02-03    CHA    DET         1.0\n",
       "17618  2023-02-03    DET    CHA         1.0\n",
       "17631  2023-02-03    MIN    ORL         1.0\n",
       "17635  2023-02-03    PHI    SAS         1.0\n",
       "17628  2023-02-03    POR    WAS         1.0\n",
       "17622  2023-02-03    SAC    IND         1.0\n",
       "17633  2023-02-03    TOR    HOU         1.0\n",
       "17630  2023-02-03    UTA    ATL         1.0\n",
       "17629  2023-02-03    ATL    UTA         0.0\n",
       "17625  2023-02-03    HOU    TOR         0.0\n",
       "17637  2023-02-03    IND    SAC         0.0\n",
       "17634  2023-02-03    ORL    MIN         0.0\n",
       "17627  2023-02-03    PHX    BOS         0.0\n",
       "17623  2023-02-03    SAS    PHI         0.0\n",
       "17609  2023-02-03    WAS    POR         0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml[(ml.actual==2) & (ml.date_next==today)][['date_next','team_x','team_y','prediction']].sort_values(['date_next','team_x']).sort_values('prediction',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4321d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
